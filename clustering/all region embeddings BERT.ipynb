{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Genome Region Description Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from visual_genome import api as vg\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import torch\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from scipy.spatial import distance_matrix, distance\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained models\n",
    "model_name='all-mpnet-base-v2'\n",
    "\n",
    "# Possible value:\n",
    "# 'all-mpnet-base-v2' : Best accuracy overall\n",
    "# 'all-MiniLM-L12-v2' : Faster\n",
    "# 'all-roberta-large-v1' : Better but slower\n",
    "\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, region_sentences):\n",
    "    sentence_embeddings = model.encode(region_sentences)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images and their descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read images and their descriptions\n",
    "image_regions = pd.read_csv(\"/home/maelic/Documents/PhD/ModelZoo/visual-genome-embeddings/create image regions/image_regions.csv\")\n",
    "image_regions.head()\n",
    "image_regions[\"region_sentences\"] = image_regions[\"region_sentences\"].apply(eval)\n",
    "regions = image_regions[\"region_sentences\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_regions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_emb_path = \"/home/maelic/Documents/PhD/MyModel/PhD_Commonsense_Enrichment/VG_refinement/clustering/sentence_embeddings/\"+model_name+\"_regions_embeddings.pkl\"\n",
    "\n",
    "if not os.path.exists(region_emb_path):\n",
    "    values = []\n",
    "    for corpus in tqdm(regions):\n",
    "        # Number of rows corresponds to number of sentences\n",
    "        if corpus:\n",
    "            emb = model.encode(corpus)\n",
    "            # get mean value columnwise, so that sentence embeddings are averaged per region for each image\n",
    "            emb = np.mean(np.array(emb), axis=0) \n",
    "            # for each model, a 768-length embedding is stored\n",
    "            values.append(emb)\n",
    "        else:\n",
    "            values.append(0)\n",
    "            print(0)\n",
    "\n",
    "    image_regions['embeddings'] = pd.Series(values, index=image_regions.index) \n",
    "    # number of sentences x 768\n",
    "    image_regions.head()\n",
    "\n",
    "    with open(region_emb_path, \"wb\") as fOut:\n",
    "        pickle.dump(image_regions,fOut,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(region_emb_path, \"rb\") as fIn:\n",
    "        image_regions = pickle.load(fIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_regions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_question(images_regions, key, NUM_CLUSTERS):\n",
    "\n",
    "    sentences = images_regions['region_sentences']\n",
    "\n",
    "    X = np.array(images_regions[key].tolist())\n",
    "\n",
    "    data = images_regions[['Image_id', 'region_sentences', key]].copy()\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(data[key], assign_clusters=True)\n",
    "\n",
    "    data['cluster'] = pd.Series(assigned_clusters, index=data.index)\n",
    "    data['centroid'] = data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return data, assigned_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_question(images_regions, key, NUM_CLUSTERS):\n",
    "\n",
    "    sentences = images_regions['region_sentences']\n",
    "\n",
    "    X = np.array(images_regions[key].tolist())\n",
    "\n",
    "    data = images_regions[['Image_id', 'region_sentences', key]].copy()\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(data[key], assign_clusters=True)\n",
    "\n",
    "    data['cluster'] = pd.Series(assigned_clusters, index=data.index)\n",
    "    data['centroid'] = data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return data, assigned_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_centroid(row):\n",
    "    # type of emb and centroid is different, hence using tolist below\n",
    "    return distance_matrix([row['embeddings']], [row['centroid'].tolist()])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_inertia(feature_matrix, centroid):\n",
    "    sum_ = []\n",
    "    for i in range(feature_matrix.shape[0]):\n",
    "        sum_.append(np.sum((feature_matrix[i] - centroid[i])**2))  \n",
    "\n",
    "    return sum(sum_) \n",
    "    \n",
    "def number_of_clusters(image_regions, key):\n",
    "    sse = []\n",
    "    list_k = list(range(2,31))\n",
    "    for k in tqdm(list_k):\n",
    "        data, assigned_clusters = clustering_question(image_regions, key, k)\n",
    "        sse.append(nltk_inertia(data[key].to_numpy(), data.centroid.to_numpy()))\n",
    "\n",
    "    # Plot sse against k\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title('Elbow method for '+model_name)\n",
    "    plt.plot(list_k, sse, '-o')\n",
    "    plt.xlabel('Number of clusters k')\n",
    "    plt.ylabel('Sum of squared distance')\n",
    "    plt.show()\n",
    "\n",
    "#number_of_clusters(image_regions, 'embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clusters(key, images_regions, n_clusters):\n",
    "    data, assigned_clusters = clustering_question(images_regions, key, NUM_CLUSTERS = n_clusters)\n",
    "    # Compute centroid distance to the data\n",
    "    data['distance_from_centroid'] = data.apply(distance_from_centroid, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_visual(embedding_clusters, key, n_clusters=0, save=False):\n",
    "    mat = np.matrix([x for x in embedding_clusters[key]])\n",
    "    t_sne = TSNE(n_components=2)\n",
    "    low_dim_data = t_sne.fit_transform(np.asarray(mat))\n",
    "    print('Lower dim data has shape',low_dim_data.shape)\n",
    "    tsne_df =  pd.DataFrame(low_dim_data, embedding_clusters['cluster'])\n",
    "    plt.figure(figsize=(20,12))\n",
    "    ax = sns.scatterplot(data=tsne_df[0], x=tsne_df[0], y=tsne_df[1], hue=tsne_df.index, palette = \"viridis\", s=80)\n",
    "    ax.set_title('T-SNE BERT Embeddings')\n",
    "    plt.draw()\n",
    "    if save:\n",
    "        plt.savefig(\"visualization/\"+model_name+\"_tsne_clusters_\"+str(n_clusters)+\".png\")\n",
    "    return low_dim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distances(images_regions, input_id, key, input_embedding = np.zeros(5)):\n",
    "    distances = []     \n",
    "    image_ids=images_regions.Image_id\n",
    "    if input_id>0:\n",
    "        reference_embedding = images_regions.loc[images_regions.Image_id == input_id][key]\n",
    "        reference_embedding = reference_embedding.values[0].reshape(-1,1)\n",
    "        corpus_embeddings = images_regions.loc[images_regions.Image_id != input_id][key]\n",
    "    else:\n",
    "        reference_embedding = input_embedding\n",
    "        corpus_embeddings = images_regions[key]\n",
    "        \n",
    "    for j in range(len(corpus_embeddings)):  # rows of def_embeddings matrix\n",
    "        defin = j\n",
    "        if image_ids[j]!=input_id:      # avoid calculating distance with itself\n",
    "            corpus = corpus_embeddings[j].reshape(-1,1)\n",
    "            # euclidean distance between multidimensional vectors\n",
    "            #dist = distance.euclidean(np.asarray(reference_embedding)[0], np.asarray(corpus)[0])\n",
    "            dist = distance.euclidean(reference_embedding.flatten(), corpus.flatten())\n",
    "\n",
    "            distances.append([image_ids[j], dist]) \n",
    "        \n",
    "    # store in df\n",
    "    col_names = ['image_id', 'distances']\n",
    "    distances_df = pd.DataFrame(distances, columns=col_names)\n",
    "    distances_df = distances_df.sort_values(by='distances', ascending=True)\n",
    "    distances_df.to_csv('distances.csv', index=False)\n",
    "    return distances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given image id to retrieve its k most similar images\n",
    "\n",
    "def retrieve_images(key, images_regions, input_id = -1, input_embedding = np.zeros(5)):\n",
    "    print('Images retrieved using method:', key)\n",
    "    top_k=10\n",
    "    \n",
    "    if input_id>0:\n",
    "    # top_k results to return\n",
    "        print('Reference image:', input_id)\n",
    "        retrieve_image(input_id)\n",
    "        distances_df=find_distances(images_regions, input_id, key)\n",
    "    else:\n",
    "        distances_df=find_distances(images_regions, input_id, key, input_embedding)\n",
    "    top_images=distances_df.head(top_k)\n",
    "\n",
    "    print(\"Top\", top_k, \"most similar images to image\", input_id, \"in Visual Genome:\")\n",
    "    for index, row in top_images.iterrows():   \n",
    "        im_id = int(row.image_id)\n",
    "        print(\"Image id:\", im_id, \"Euclidean distance: %.4f\" % (row.distances))\n",
    "\n",
    "        # find similar images from api and show\n",
    "        retrieve_image(im_id)\n",
    "\n",
    "    return top_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method: stsb-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_method = \"embeddings\"\n",
    "num_clusters = 9\n",
    "\n",
    "embedding_clusters=make_clusters(embeddings_method, image_regions, num_clusters)\n",
    "print('Number of clusters:', num_clusters)\n",
    "labels = embedding_clusters['cluster'].to_numpy()\n",
    "#print(embedding_clusters.groupby('cluster').count())\n",
    "low_embedding_clusters = tsne_visual(embedding_clusters, embeddings_method, n_clusters=num_clusters, save=True)\n",
    "\n",
    "silhouette_vals = silhouette_samples(low_embedding_clusters, labels)\n",
    "avg_score = silhouette_score(low_embedding_clusters, labels)\n",
    "print(\"Average silhouette score:\", avg_score)\n",
    "\n",
    "# plt.figure(figsize=(20,12))\n",
    "\n",
    "# # Silhouette plot\n",
    "# y_ticks = []\n",
    "# y_lower, y_upper = 0, 0\n",
    "# for i, cluster in enumerate(np.unique(labels)):\n",
    "#     cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "#     cluster_silhouette_vals.sort()\n",
    "#     y_upper += len(cluster_silhouette_vals)\n",
    "#     plt.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "#     plt.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "#     y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "# avg_score = np.mean(silhouette_vals)\n",
    "# plt.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "# plt.yticks([])\n",
    "# plt.xlim([-0.1, 1])\n",
    "# plt.xlabel('Silhouette coefficient values')\n",
    "# plt.ylabel('Cluster labels')\n",
    "# plt.title(f'Silhouette analysis using k = {i+1}', y=1.02)\n",
    "# # title=f'Silhouette analysis using k = {i}, average score = {avg_score:.3f}'\n",
    "# # plt.suptitle(title, fontsize=16, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = silhouette_score(low_embedding_clusters, labels)\n",
    "print(\"Average silhouette score:\", avg_score)\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "# Silhouette plot\n",
    "y_ticks = []\n",
    "y_lower, y_upper = 0, 0\n",
    "for i, cluster in enumerate(np.unique(labels)):\n",
    "    cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    y_upper += len(cluster_silhouette_vals)\n",
    "    plt.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "    plt.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "    y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "avg_score = np.mean(silhouette_vals)\n",
    "plt.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "plt.yticks([])\n",
    "plt.xlim([-0.1, 1])\n",
    "plt.xlabel('Silhouette coefficient values')\n",
    "plt.ylabel('Cluster labels')\n",
    "plt.title(f'Silhouette analysis using k = {i+1}', y=1.02)\n",
    "# # title=f'Silhouette analysis using k = {i}, average score = {avg_score:.3f}'\n",
    "# # plt.suptitle(title, fontsize=16, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_clusters.keys())\n",
    "print(embedding_clusters['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_clusters.keys())\n",
    "i=0\n",
    "img_clusters = embedding_clusters[['Image_id','cluster']].copy()\n",
    "print(img_clusters.keys())\n",
    "print(img_clusters['cluster'].value_counts())\n",
    "img_clusters.to_csv('img_clusters.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize image from cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_regions(image, regions):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for region in regions:\n",
    "        ax.add_patch(Rectangle((region['x'], region['y']),\n",
    "                               region['width'],\n",
    "                               region['height'],\n",
    "                               fill=False,\n",
    "                               edgecolor='red',\n",
    "                               linewidth=3))\n",
    "        ax.text(region['x'], region['y'], region['phrase'], style='italic', bbox={'facecolor':'white', 'alpha':0.7, 'pad':10})\n",
    "    fig = plt.gcf()\n",
    "    plt.tick_params(labelbottom='off', labelleft='off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image_regions(vg_img_path, regions, image_id):\n",
    "    # read image from disk using image path\n",
    "    image = Image.open(os.path.join(vg_img_path, str(image_id) + '.jpg'))\n",
    "    # get regions of image\n",
    "    if image:\n",
    "        for r in regions:\n",
    "            if r['id'] == image_id:\n",
    "                reg = r['regions']\n",
    "                break\n",
    "        # show images\n",
    "        visualize_regions(image, reg[:8])    # call with fewer regions for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image samples from selected cluster\n",
    "import random\n",
    "\n",
    "img_clusters = pd.read_csv('img_clusters.csv')\n",
    "vg_img_path = '/home/maelic/Documents/PhD/Datasets/VisualGenome/VG_100K'\n",
    "region_path = '/home/maelic/Documents/PhD/Datasets/VisualGenome/original_annotations/region_descriptions.json'\n",
    "regions = json.load(open(region_path, 'r'))\n",
    "cluster_id = 8\n",
    "nb_img = 5\n",
    "cluster_images = img_clusters[img_clusters['cluster']==cluster_id]\n",
    "print(len(cluster_images))\n",
    "for i in range(nb_img):\n",
    "    random_id = random.randint(0, len(cluster_images))\n",
    "    show_image_regions(vg_img_path, regions, cluster_images.iloc[random_id,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17740\n"
     ]
    }
   ],
   "source": [
    "indoor_vg = img_clusters[img_clusters['cluster']==5]\n",
    "print(len(indoor_vg))\n",
    "indoor_vg.to_csv('indoor_vg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_method = \"embeddings_4\"\n",
    "clusters_max = 21\n",
    "\n",
    "score_list = {}\n",
    "\n",
    "for i in range(15, clusters_max):\n",
    "    embedding_clusters=make_clusters(embeddings_method, image_regions, i)\n",
    "    print('Number of clusters:', i)\n",
    "    print(embedding_clusters.groupby('cluster').count())\n",
    "    low_embedding_clusters = tsne_visual(embedding_clusters, embeddings_method, n_clusters=i, save=True)\n",
    "\n",
    "    # Get silhouette samples\n",
    "    low_embedding_clusters = low_embedding_clusters\n",
    "    embedding_clusters = embedding_clusters\n",
    "\n",
    "    plt.figure(figsize=(20,12))\n",
    "\n",
    "    embedding_samples = low_embedding_clusters\n",
    "    labels = embedding_clusters['cluster'].to_numpy()\n",
    "    silhouette_vals = silhouette_samples(embedding_samples, labels)\n",
    "    avg_score = silhouette_score(embedding_samples, labels)\n",
    "    print(f\"For n_clusters = {i} The average silhouette_score is : {avg_score}\")\n",
    "    score_list[i] = {'score': avg_score, 'silhouette_vals': silhouette_vals}\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        plt.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        plt.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    plt.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    plt.yticks([])\n",
    "    plt.xlim([-0.1, 1])\n",
    "    plt.xlabel('Silhouette coefficient values')\n",
    "    plt.ylabel('Cluster labels')\n",
    "    plt.title(f'Silhouette analysis using k = {i}', y=1.02)\n",
    "    # title=f'Silhouette analysis using k = {i}, average score = {avg_score:.3f}'\n",
    "    # plt.suptitle(title, fontsize=16, y=1.05)\n",
    "    plt.draw()\n",
    "    plt.savefig(\"visualization/\"+model_name+\"_silhouette_clusters_embeddings_\"+str(i)+\".png\")\n",
    "\n",
    "# write to log file:\n",
    "with open('visualization/'+model_name+'__clusters_log_'+key+'.txt', 'w') as f:\n",
    "    for key, value in score_list.items():\n",
    "        f.write('%s: %s \\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_clusters.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar image retrieval based on given image id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = 1\n",
    "top_images = retrieve_images(embeddings_method, image_regions, input_id = input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar image retrieval based on given user sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'a cat is sleeping on the beach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = get_embeddings(model_4, input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_images_2 = retrieve_images(embeddings_method, image_regions, input_id = -1, input_embedding=input_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_images_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7938123ceb831a528202b02dbdb8fd920084013224d6ffd4fcb9a5f6f5074515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
